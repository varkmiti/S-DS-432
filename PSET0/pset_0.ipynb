{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSET 0\n",
    "## Mark Viti\n",
    "### Due: Feb. 1, 2024\n",
    "\n",
    "## Problem 1\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be a be a differentiable convex function. We claim that the gradient $\\nabla f$ is monotone. \n",
    "\n",
    "*Proof.*\n",
    "First, let us recall the definition of monotone as given by Boyd. For a function $\\psi$ to be monotone, it must be the case that for all $x, y \\in \\text{dom}(\\psi)$, we have that\n",
    "$\\begin{align}\n",
    "\\left(\\psi(x) - \\psi(y)\\right)^T (x - y) \\geq 0\n",
    "\\end{align}$\n",
    "Now, let us consider the first order condition for convexity. That is, we have that\n",
    "$\\begin{align}\n",
    "f(y) \\geq f(x) + \\nabla f(x)^T (y - x)\n",
    "\\end{align}$\n",
    "and \n",
    "$\\begin{align}\n",
    "f(x) \\geq f(y) + \\nabla f(y)^T (x - y)\n",
    "\\end{align}$\n",
    "Thus, all we need to do is combine these two inequalities to get the desired result. To that end, we have that\n",
    "$\\begin{align}\n",
    "f(y) - \\left[f(y) + \\nabla f(y)^T (x - y)\\right] & \\geq f(x) + \\nabla f(x)^T (y - x) - \\left[f(x)\\right] \\\\\n",
    "0 & \\geq \\nabla f(x)^T (y - x) - \\nabla f(y)^T (x - y) \\\\\n",
    "0 & \\leq \\left[\\nabla f(x) - \\nabla f(y)\\right]^T (x - y)\n",
    "\\end{align}$\n",
    "And so we have that $\\nabla f$ is monotone. $\\blacksquare$\n",
    "\n",
    "To show the second part of the problem, we will consider the function $f(x) = \\left(x_1, \\frac{x_1}{2} + x_2\\right)^T$. We will first show that the function is monotone. Note then that we have that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be the log-sum-exp function defined as\n",
    "$\\begin{align}\n",
    "f(x) = \\log \\left(\\sum_{i=1}^n e^{x_i}\\right)\n",
    "\\end{align}$\n",
    "\n",
    "We will show that $f$ is convex by showing that the Hessian is positive semidefinite.\n",
    "\n",
    "*Proof.*\n",
    "First, let us compute the Hessian of $f$. We will compute the Hessian by first computing the gradient of $f$. To that end, we have that\n",
    "$\\begin{align}\n",
    "LSE(x) & = \\log \\left(\\sum_{i=1}^n e^{x_i}\\right) \\\\\n",
    "\\nabla LSE(x) & = \\frac{1}{\\sum_{i = 1}^{n} e^{x_i}}\\begin{pmatrix} e^{x_1} \\\\ e^{x_2} \\\\ \\vdots \\\\ e^{x_n} \\end{pmatrix} && {\\text{By the Chain Rule}} \\\\\n",
    "\\end{align}$\n",
    "Now, to find the Hessian we will need to find the gradient of each element of the vector $\\nabla LSE(x)$. Let us denote the $i$th element of $\\nabla LSE(x)$ as $lse_i(x)$. For clarity, this means that \n",
    "$\\begin{align}\n",
    "lse_i(x) = \\frac{e^{x_i}}{\\sum_{i = 1}^{n} e^{x_i}}\n",
    "\\end{align}$\n",
    "Now, we can make a useful simplification by saying that $a_i = e^{x_i}$ and that $A = \\sum_{i = 1}^{n} e^{x_i}$. With this notation, we have that\n",
    "$\\begin{align}\n",
    "lse_i(x) = \\frac{a_i}{A}\n",
    "\\end{align}$\n",
    "And then we have two cases. First, let us consider the case such that $i = j$. In this case, we have that\n",
    "$\\begin{align}\n",
    "\\frac{\\partial lse_i(x)}{\\partial x_i} & = \\frac{A \\frac{\\partial a_i}{\\partial x_i} - a_i \\frac{\\partial A}{\\partial x_i}}{A^2} \\\\\n",
    "& = \\frac{a_i}{A} - \\frac{a_i^2}{A^2}\n",
    "\\end{align}$\n",
    "And then if $i \\neq j$, we have that\n",
    "$\\begin{align}\n",
    "\\frac{\\partial lse_i(x)}{\\partial x_j} & = \\frac{A \\frac{\\partial a_i}{\\partial x_j} - a_i \\frac{\\partial A}{\\partial x_j}}{A^2} \\\\\n",
    "& = -\\frac{a_i a_j}{A^2}\n",
    "\\end{align}$\n",
    "Now, we want to write this in a more compact form. For simplicity again, let us call $a = (e^{x_1}, \\dots e^{x_n})$\n",
    "$\\begin{align}\n",
    "\\nabla^2LSE(x) & = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\left(\\sum_{i = 1}^n e^{x_i}\\right)\\textbf{diag}(a) - aa^T\\right] \\\\\n",
    "\\end{align}$\n",
    "Now, we want to show that this is positive semidefinite. To that end, let us consider the following vector $v \\in \\mathbb{R}^n$.\n",
    "$\\begin{align}\n",
    "v^T \\nabla^2LSE(x) v & = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\left(\\sum_{i = 1}^n e^{x_i}\\right)v^T\\textbf{diag}(a)v - v^Taa^Tv\\right] \\\\\n",
    "& = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\sum_{i = 1}^n e^{x_i}v_i^2 - \\sum_{i = 1}^n \\sum_{j = 1}^n e^{x_i}e^{x_j}v_iv_j\\right] \\\\\n",
    "\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "We are going to show that the information inequality holds for the KL divergence. Due to the hint, we know that the negative log function is convex. Thus, let us recall the definition of the KL divergence. We have that for all $u, v \\in \\mathbb{R}^n_{++}$:\n",
    "$\\begin{align}\n",
    "D_{kl}(u, v) & = f(u) - f(v) - \\nabla f(v)^T(u - v)\n",
    "\\end{align}$\n",
    "Such that $f(v) - \\sum_{i = 1}^n v_i \\log v_i$ is the negative log function. Now, since the negative log function is convex, we have that\n",
    "$\\begin{align}\n",
    "f(u) & \\geq f(v) + \\nabla f(v)^T(u - v) \\\\\n",
    "f(u) - f(v) & \\geq \\nabla f(v)^T(u - v) \\\\\n",
    "f(u) - f(v) - \\nabla f(v)^T(u - v) & \\geq 0 \\\\\n",
    "D_{kl}(u, v) & \\geq 0\n",
    "\\end{align}$\n",
    "Now, we want to show that the inequality is strict if and only if $u = v$. To that end, let us consider the case where $u = v$. In this case, we have that\n",
    "$\\begin{align}\n",
    "D_{kl}(u, v) & = f(u) - f(v) - \\nabla f(v)^T(u - v) \\\\\n",
    "& = f(u) - f(u) - \\nabla f(u)^T(u - u) \\\\\n",
    "& = 0\n",
    "\\end{align}$\n",
    "Now, if $D_{kl}(u, v), D_{kl}(v, u) = 0$, then we have that\n",
    "$\\begin{align}\n",
    "D_{kl}(u, v) & = f(u) - f(v) - \\nabla f(v)^T(u - v) \\\\\n",
    "D_{kl}(v, u) & = f(v) - f(u) - \\nabla f(u)^T(v - u) \\\\\n",
    "D_{kl}(u, v) + D_{kl}(v, u) & = f(u) - f(v) - \\nabla f(v)^T(u - v) + f(v) - f(u) - \\nabla f(u)^T(v - u) \\\\\n",
    "0 & = - \\nabla f(v)^T(u - v) - \\nabla f(u)^T(v - u) \\\\\n",
    "0 & = \\nabla f(v)^T(u - v) + \\nabla f(u)^T(v - u) \\\\\n",
    "0 & = \\nabla f(v)^T(u - v) - \\nabla f(u)^T(u - v) \\\\\n",
    "0 & = \\left[\\nabla f(v) - \\nabla f(u)\\right]^T(u - v)\n",
    "\\implies \\nabla f(v) = \\nabla f(u) \\text{ or } u = v\n",
    "\\end{align}$\n",
    "Now, we know that the gradient of the negative log function is given by\n",
    "$\\begin{align}\n",
    "\\nabla f(v) & = \\begin{pmatrix} -\\frac{1}{v_1} \\\\ -\\frac{1}{v_2} \\\\ \\vdots \\\\ -\\frac{1}{v_n} \\end{pmatrix}\n",
    "\\end{align}$\n",
    "Thus, we have that $\\nabla f(v) = \\nabla f(u)$ if and only if $u = v$. Thus, we have that $D_{kl}(u, v) = 0$ if and only if $u = v$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "**Additional Questions**\n",
    "### (a)\n",
    "Let us define Bregman divergence as follows:\n",
    "$\\begin{align}\n",
    "D_f(x, y) & = f(x) - f(y) - \\nabla f(y)^T(x - y)\n",
    "\\end{align}$\n",
    "Such that $f$ is the quadratic function $f(x) = \\vert\\vert x\\vert\\vert_2^2$. Then, we claim that $D_f(x, y) = \\vert\\vert x - y\\vert\\vert_2^2$.\n",
    "\n",
    "*Proof.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate $m$ data points $\\{y_i, x_i\\}$ satisfying $y_i = x_i^\\top \\beta$, where $\\beta \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 432\n",
    "np.random.seed(seed)\n",
    "def generate_data(m, n):\n",
    "    \"\"\"\n",
    "    Generate data for a noiseless linear model y = X beta.\n",
    "\n",
    "    Parameters:\n",
    "    - m: Number of data points (rows of X).\n",
    "    - n: Number of features (columns of X and size of beta).\n",
    "\n",
    "    Returns:\n",
    "    - X: Random matrix of size m x n.\n",
    "    - y: Vector of size m obtained using the linear model.\n",
    "    - beta: Gaussian random vector of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random matrix X of size m x n\n",
    "    X = np.random.rand(m, n)\n",
    "\n",
    "    # Generate Gaussian random vector beta of size n\n",
    "    beta = np.random.randn(n)\n",
    "\n",
    "    # Compute y using the linear model\n",
    "    y = X @ beta\n",
    "\n",
    "    return X, y, beta\n",
    "\n",
    "# Example usage:\n",
    "# m, n = 100, 5\n",
    "# X, y, beta = generate_data(m, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the rank deficient case where $m < n$. Show that there are multiple solutions to the linear equation $y = X \\beta$.\n",
    "\n",
    "We generate some data $X, y$ with $m = 50$ and $n = 200$. We can see that $y = X b$ has at least two solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 and b2 are solutions to $y = X *beta$ because the residue are 3.049105001863942e-13,2.554003338504377e-13\n"
     ]
    }
   ],
   "source": [
    "def find_two_solutions_corrected(X, y):\n",
    "    # Compute the pseudo-inverse of X\n",
    "    X_pseudo = np.linalg.pinv(X)\n",
    "\n",
    "    # Compute the minimum norm solution\n",
    "    beta_min_norm = np.dot(X_pseudo, y)\n",
    "\n",
    "    # Compute a basis for the null space of X\n",
    "    _, _, Vt = np.linalg.svd(X)\n",
    "    null_space_basis = Vt[-(X.shape[1]-np.linalg.matrix_rank(X)):].T\n",
    "\n",
    "    # Generate a vector in the null space of X\n",
    "    arbitrary_coefficients = np.random.rand(null_space_basis.shape[1])\n",
    "    null_space_vector = np.dot(null_space_basis, arbitrary_coefficients)\n",
    "\n",
    "    # Compute the arbitrary solution\n",
    "    beta_arbitrary = beta_min_norm + null_space_vector\n",
    "\n",
    "    return beta_min_norm, beta_arbitrary\n",
    "\n",
    "m = 100\n",
    "n = 400\n",
    "X, y, beta = generate_data(m, n)\n",
    "b1, b2 = find_two_solutions_corrected(X, y)\n",
    "\n",
    "print(f\"b1 and b2 are solutions to $y = X *beta$ because the residue are {np.linalg.norm(y - X@b1)},{np.linalg.norm(y - X@b2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-norm solution via cvxpy\n",
    "\n",
    "The mathematical formulation for finding the minimum norm solution $ \\mathbf{b} $ subject to the constraint $ y = X \\mathbf{b} $ is as follows:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad & \\lVert \\mathbf{b} \\rVert_2 \\\\\n",
    "\\text{subject to} \\quad & X \\mathbf{b} = y\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $ \\lVert \\mathbf{b} \\rVert_2 $ is the Euclidean norm of $ \\mathbf{b} $.\n",
    "- $ X $ is the given matrix.\n",
    "- $ y $ is the given vector.\n",
    "\n",
    "\n",
    "\n",
    "Once you run this code in your local environment with CVXPY installed, it will return the minimum norm solution $ \\mathbf{b} $ for the equation $ y = X \\mathbf{b} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is a solution to $y = X *beta$ because the residue is 1.1427512163646394e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markviti/opt/anaconda3/envs/data/lib/python3.10/site-packages/cvxpy/reductions/solvers/solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def min_norm_solution_cvxpy(X, y):\n",
    "    # Define the variable\n",
    "    b = cp.Variable(X.shape[1])\n",
    "\n",
    "    # Complete this function\n",
    "    # Define the objective\n",
    "    objective = cp.Minimize(cp.norm(b, 2))\n",
    "\n",
    "    # Define the constraints\n",
    "    constraints = [X @ b == y]\n",
    "\n",
    "    # Create the problem and solve it\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    return b.value\n",
    "\n",
    "b = min_norm_solution_cvxpy(X, y)\n",
    "print(f\"b is a solution to $y = X *beta$ because the residue is {np.linalg.norm(y - X@b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent starting from zero.\n",
    "\n",
    " The gradient descent algorithm updates the parameters iteratively using the gradient of the objective function with respect to the parameters.\n",
    "\n",
    "For the linear system $ y = X \\beta $ and the objective function $J(\\beta) = \\frac{1}{m}\\lVert X \\beta - y \\rVert_2^2 $, the gradient with respect to $ \\beta $ is:\n",
    "$$\n",
    "\\nabla J(\\beta) = \\frac{2}{m} X^T (X \\beta - y)\n",
    "$$\n",
    "\n",
    "Using the gradient descent update rule, we can iteratively update the solution $ \\beta $ as:\n",
    "\n",
    "$$\n",
    "\\beta_{\\text{new}} = \\beta_{\\text{old}} - \\alpha \\nabla J(\\beta)\n",
    "$$\n",
    "\n",
    "Where $ \\alpha $ is the learning rate. You are going to implement gradient descent, initialized from a zero vector.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate = 0.001, num_iterations = 10000):\n",
    "    # Initialize beta as a zero vector\n",
    "    beta = np.zeros(X.shape[1])\n",
    "\n",
    "    # Perform gradient descent\n",
    "    for t in range(num_iterations):\n",
    "        loss_value, grad_norm = loss_and_gradient(X, y, beta)\n",
    "\n",
    "        beta = beta - learning_rate * grad_norm\n",
    "\n",
    "    return beta\n",
    "\n",
    "def loss_and_gradient(X, y, beta):\n",
    "    # compute least-squres loss and the norm of gradient\n",
    "    grad_norm = 2/X.shape[0] * X.T @ (X @ beta - y)\n",
    "    loss = np.linalg.norm(X @ beta - y)**2\n",
    "    return loss, grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test thse two functions. We want to show that gradient descent finds the min-norm solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference between the two solutions are bounded by 9.40898184878866e-09\n",
      "gradient descent with a very small learning rate coincides with the min-norm solution\n",
      "the loss and gradient of loss of GD solution is (3.7326855852289988e-16, array([-3.45787555e-11,  1.87498045e-12,  2.19052477e-11,  5.41291469e-11,\n",
      "        8.10265126e-11, -6.71255989e-11,  1.54865667e-11, -4.60151769e-11,\n",
      "        1.06811038e-10,  2.01946858e-12, -7.75006042e-11, -2.06031065e-11,\n",
      "        6.11811574e-12, -7.07058071e-11, -4.89753733e-11,  3.83058510e-11,\n",
      "       -4.01253420e-12, -4.12629302e-11, -2.54371349e-11, -3.50446351e-11,\n",
      "        8.05248713e-11,  3.16513814e-11, -2.01335399e-11, -3.30012027e-11,\n",
      "       -4.90791139e-11, -1.55308653e-10, -2.87315929e-11, -1.85749911e-10,\n",
      "        1.31246944e-10, -1.05758916e-10, -4.23603822e-11,  6.71450395e-11,\n",
      "       -6.56058062e-11, -7.16361190e-11, -5.82282568e-11, -1.34370069e-11,\n",
      "        1.46278732e-11,  9.73296576e-11, -1.93937200e-11, -1.55175058e-11,\n",
      "       -1.34346562e-10,  7.57277953e-11, -3.41892937e-11, -6.63680980e-11,\n",
      "       -5.56919782e-11, -7.33934154e-11,  8.23265813e-12,  1.31337422e-10,\n",
      "       -1.58013968e-11,  6.64332234e-11,  3.41479626e-11, -2.08842063e-11,\n",
      "       -2.21918275e-11,  2.23322076e-11,  4.54600740e-11, -3.63135408e-11,\n",
      "       -2.53453541e-11,  4.26219983e-12,  3.93371202e-11,  1.52341397e-11,\n",
      "        7.04684516e-11,  3.36496411e-11, -1.05560879e-11,  4.17901614e-11,\n",
      "       -5.16249601e-11,  1.25858264e-12, -1.39341168e-11, -4.31292019e-11,\n",
      "        6.79216801e-11,  3.10635184e-11,  5.15291244e-11,  1.09791973e-10,\n",
      "        9.21053465e-11, -6.71945260e-11, -6.80035543e-12, -7.55817336e-11,\n",
      "       -1.11439636e-11, -1.90376755e-11, -2.17102301e-11, -6.40714906e-11,\n",
      "       -1.31598699e-10,  2.43380326e-11, -1.82698206e-11, -5.10360463e-11,\n",
      "       -2.71984694e-11,  3.23719149e-11,  2.38419820e-11,  5.58172623e-11,\n",
      "       -5.85937970e-11,  1.08253278e-11, -9.06843788e-12,  7.29951806e-11,\n",
      "       -1.87204826e-11, -1.95038011e-11,  1.56515486e-11, -8.84928959e-11,\n",
      "       -4.72033382e-11,  1.89596204e-11,  7.69140208e-12,  1.02168885e-12,\n",
      "        6.72662179e-11, -3.91634345e-11,  2.19576430e-11,  5.73051663e-12,\n",
      "        4.66969703e-11, -1.05789592e-12, -1.12277195e-10, -4.65252084e-11,\n",
      "       -2.57278474e-11, -4.37363208e-11, -4.00273743e-11, -5.91093947e-11,\n",
      "        6.92239242e-11,  2.02218052e-11,  3.78616046e-11,  1.93346806e-11,\n",
      "        6.37626294e-11,  1.30826442e-11,  3.90069369e-11, -4.08627954e-11,\n",
      "       -2.52754580e-12, -1.25622227e-10,  4.39045521e-11,  1.75373363e-11,\n",
      "        9.56982368e-11,  1.51705826e-11, -8.82916569e-12, -1.14754573e-12,\n",
      "       -9.96652087e-11,  8.97665324e-11, -7.92993893e-11,  7.19717898e-11,\n",
      "       -6.81417614e-11,  5.06560600e-11,  2.18881205e-11, -2.26136259e-11,\n",
      "       -1.36883985e-11, -1.00113332e-11, -6.36281835e-11, -9.10034238e-12,\n",
      "       -1.22658351e-11, -1.09635168e-10,  4.13647774e-12,  1.59944568e-10,\n",
      "       -2.03018686e-11, -6.62491627e-11,  3.67216768e-11,  4.75362596e-11,\n",
      "        6.54089822e-11, -3.38686882e-12, -3.31378119e-11, -6.58433793e-11,\n",
      "        1.60828507e-10,  5.07298647e-11,  1.84529382e-12,  3.15602430e-11,\n",
      "       -4.96121698e-11,  3.06417447e-11,  5.43608204e-11,  1.65989623e-11,\n",
      "        2.76414073e-12, -7.45176043e-11,  1.10039863e-10, -6.05361162e-11,\n",
      "       -1.38504955e-11, -1.54797376e-11,  3.09903552e-11, -8.24652512e-11,\n",
      "        1.65971504e-12, -3.10697646e-12, -2.25663894e-11,  8.32256859e-12,\n",
      "        5.12627984e-11,  6.16782320e-11, -6.72562628e-11, -2.95913551e-11,\n",
      "       -2.00760282e-11, -3.66267805e-11,  4.96268940e-11,  3.10599948e-11,\n",
      "        1.64819715e-11,  1.70005528e-11,  9.73139041e-12,  1.43213764e-11,\n",
      "       -2.88565830e-11, -5.26282558e-11, -8.07155559e-12, -6.94154196e-11,\n",
      "        1.03964238e-10, -8.57521838e-11,  3.57320949e-13, -3.43585894e-11,\n",
      "        2.98278435e-11,  3.19451845e-11,  4.84667423e-11,  3.23774109e-11,\n",
      "       -8.85630515e-11,  4.91721948e-11,  2.73268890e-11,  3.27069569e-11,\n",
      "       -1.07480118e-10,  5.28811026e-12, -6.41145885e-11,  7.89314153e-12,\n",
      "        3.54522757e-11, -5.29633737e-11,  1.02597546e-10,  3.38339540e-11,\n",
      "        3.00364553e-11,  7.17789669e-11,  3.33807115e-11,  1.02199626e-11,\n",
      "       -7.24422756e-11, -6.23297917e-11,  3.36800329e-12,  5.75673118e-11,\n",
      "       -1.98174071e-11,  1.07375577e-10,  1.78044313e-11,  3.56005589e-12,\n",
      "       -5.63922540e-12,  4.23210587e-11, -6.25782930e-11,  6.17299364e-11,\n",
      "        1.35932210e-10,  3.81154151e-12, -8.52544915e-11, -4.57057374e-12,\n",
      "       -2.48871486e-11,  6.33833067e-11,  1.40440110e-10, -6.80142332e-11,\n",
      "        5.27906237e-11, -4.41473991e-12,  4.44316719e-11,  9.85773857e-13,\n",
      "       -2.49520513e-11, -1.23973427e-10, -6.25760083e-11,  3.88915590e-11,\n",
      "       -4.64450920e-11, -2.53556453e-11, -1.18231938e-10,  2.62680853e-11,\n",
      "        4.20478901e-11,  1.70246578e-10, -5.20585975e-11,  3.28612804e-11,\n",
      "       -3.88046899e-11, -8.25624793e-11, -6.86958536e-11, -2.76000599e-11,\n",
      "       -2.81620733e-11,  2.26128673e-11, -4.48596058e-11, -1.63159424e-10,\n",
      "       -4.43907944e-11,  1.25682429e-11,  8.00645910e-11, -8.08838576e-11,\n",
      "       -8.39339142e-11, -1.42574657e-11, -4.06959704e-11, -1.28873426e-10,\n",
      "       -3.83885924e-11,  5.00650808e-11,  5.80643158e-11, -2.75403456e-11,\n",
      "        2.00966031e-11,  5.65169258e-11,  9.03237849e-11,  1.94326606e-11,\n",
      "        8.69126942e-12,  1.08561818e-10, -6.73756285e-11,  3.10337465e-11,\n",
      "       -6.31273780e-11,  1.05673910e-10, -2.40428921e-11,  3.97305860e-11,\n",
      "        4.15189189e-11, -2.26404509e-11,  7.63914907e-11, -2.87857160e-11,\n",
      "        1.01111717e-11,  1.04272160e-11, -6.42174275e-11, -9.57990195e-11,\n",
      "        6.41345973e-11,  5.17349186e-11, -1.50303919e-11, -9.92396607e-11,\n",
      "        6.94146938e-11, -6.00887061e-11,  6.35250312e-12,  5.12837680e-11,\n",
      "       -4.42014116e-11, -3.54877045e-11,  1.12991602e-10, -3.33358177e-11,\n",
      "        4.47507267e-11, -5.22939101e-11,  3.77054672e-11, -2.66994007e-11,\n",
      "        1.98787722e-11,  2.54804157e-11, -7.83671527e-11,  8.56891952e-12,\n",
      "       -3.12566130e-11,  5.80679428e-11, -5.63557545e-11, -3.07619268e-11,\n",
      "        1.01614964e-10, -1.03000294e-11,  1.44169883e-10,  9.52874276e-11,\n",
      "       -1.31683450e-11, -4.11417850e-11,  1.46259687e-10, -1.12441181e-11,\n",
      "        4.82664038e-11,  5.76291160e-11,  1.26174173e-10,  7.53190371e-11,\n",
      "        6.26345372e-11,  1.98221081e-12,  1.86471258e-11,  7.28248203e-11,\n",
      "       -8.09822640e-11,  6.65503305e-11, -2.05361681e-11, -7.33398198e-11,\n",
      "       -6.14369183e-12,  1.70220918e-11, -4.14388027e-11,  1.22009480e-11,\n",
      "       -1.29526172e-10,  3.85749399e-11,  1.01945570e-10,  3.31973918e-11,\n",
      "        2.02799601e-11,  1.95429159e-11, -6.94509275e-11, -3.97046692e-11,\n",
      "        1.32323216e-10, -4.81582330e-11, -1.56269744e-11, -1.75001310e-11,\n",
      "        1.50294799e-11, -6.38868658e-11,  7.76874414e-12,  5.91267096e-11,\n",
      "       -2.62295649e-11, -2.00464982e-11, -1.33544659e-11, -6.97310245e-11,\n",
      "        1.62307039e-12,  8.40746143e-11,  2.66269198e-11,  5.46156743e-11,\n",
      "       -1.56018211e-11,  3.08254815e-11,  2.92245678e-11,  1.04895005e-11,\n",
      "       -5.49009411e-11,  4.69056241e-11, -6.79305907e-11,  8.65864834e-11,\n",
      "       -6.87177210e-11, -2.80902391e-11, -1.53322393e-11,  3.88085987e-11,\n",
      "        5.02071168e-11,  5.78713035e-11, -2.87705711e-11, -1.14057916e-10,\n",
      "        3.65032873e-11, -6.37990465e-11, -4.50518833e-11, -6.40912720e-12,\n",
      "        4.20289319e-11, -2.44986647e-11,  3.06455080e-11, -4.17232528e-11,\n",
      "       -3.86569123e-11,  1.22798595e-10, -6.33821665e-11,  1.02595926e-10,\n",
      "        3.76328473e-11, -1.43000353e-10, -1.59027521e-11, -3.72458875e-12,\n",
      "        3.14095226e-11,  8.10727771e-11, -7.04669136e-11,  2.10445346e-11,\n",
      "       -6.05303917e-11,  3.40293723e-11, -9.13139181e-11,  2.36080176e-11]))\n"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "n = 400\n",
    "X, y, beta = generate_data(m, n)\n",
    "\n",
    "beta_min_norm = min_norm_solution_cvxpy(X,y)\n",
    "beta_gd = gradient_descent(X, y, learning_rate = 0.002,num_iterations= 50000)\n",
    "error = np.linalg.norm(beta_gd - beta_min_norm)\n",
    "if error < 1e-5:\n",
    "    print(f\"the difference between the two solutions are bounded by {error}\")\n",
    "    print(\"gradient descent with a very small learning rate coincides with the min-norm solution\")\n",
    "    print(f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd)}\")\n",
    "else:\n",
    "    print(f\"the difference between the two solutions are bounded by {error}\")\n",
    "    print(\"gradient descent does not coincide with min-norm solution\")\n",
    "    print(f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
