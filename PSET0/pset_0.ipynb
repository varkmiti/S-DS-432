{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSET 0\n",
    "## Mark Viti\n",
    "### Due: Feb. 1, 2024\n",
    "\n",
    "## Problem 1\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be a be a differentiable convex function. We claim that the gradient $\\nabla f$ is monotone. \n",
    "\n",
    "*Proof.*\n",
    "First, let us recall the definition of monotone as given by Boyd. For a function $\\psi$ to be monotone, it must be the case that for all $x, y \\in \\text{dom}(\\psi)$, we have that\n",
    "$\\begin{align}\n",
    "\\left(\\psi(x) - \\psi(y)\\right)^T (x - y) \\geq 0\n",
    "\\end{align}$\n",
    "Now, let us consider the first order condition for convexity. That is, we have that\n",
    "$\\begin{align}\n",
    "f(y) \\geq f(x) + \\nabla f(x)^T (y - x)\n",
    "\\end{align}$\n",
    "and \n",
    "$\\begin{align}\n",
    "f(x) \\geq f(y) + \\nabla f(y)^T (x - y)\n",
    "\\end{align}$\n",
    "Thus, all we need to do is combine these two inequalities to get the desired result. To that end, we have that\n",
    "$\\begin{align}\n",
    "f(y) - \\left[f(y) + \\nabla f(y)^T (x - y)\\right] & \\geq f(x) + \\nabla f(x)^T (y - x) - \\left[f(x)\\right] \\\\\n",
    "0 & \\geq \\nabla f(x)^T (y - x) - \\nabla f(y)^T (x - y) \\\\\n",
    "0 & \\leq \\left[\\nabla f(x) - \\nabla f(y)\\right]^T (x - y)\n",
    "\\end{align}$\n",
    "And so we have that $\\nabla f$ is monotone. $\\blacksquare$\n",
    "\n",
    "To show the second part of the problem, we will consider the function $f(x) = \\left(x_1, \\frac{x_1}{2} + x_2\\right)^T$. We will first show that the function is monotone. Note then that we have that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be the log-sum-exp function defined as\n",
    "$\\begin{align}\n",
    "f(x) = \\log \\left(\\sum_{i=1}^n e^{x_i}\\right)\n",
    "\\end{align}$\n",
    "\n",
    "We will show that $f$ is convex by showing that the Hessian is positive semidefinite.\n",
    "\n",
    "*Proof.*\n",
    "First, let us compute the Hessian of $f$. We will compute the Hessian by first computing the gradient of $f$. To that end, we have that\n",
    "$\\begin{align}\n",
    "LSE(x) & = \\log \\left(\\sum_{i=1}^n e^{x_i}\\right) \\\\\n",
    "\\nabla LSE(x) & = \\frac{1}{\\sum_{i = 1}^{n} e^{x_i}}\\begin{pmatrix} e^{x_1} \\\\ e^{x_2} \\\\ \\vdots \\\\ e^{x_n} \\end{pmatrix} && {\\text{By the Chain Rule}} \\\\\n",
    "\\end{align}$\n",
    "Now, to find the Hessian we will need to find the gradient of each element of the vector $\\nabla LSE(x)$. Let us denote the $i$th element of $\\nabla LSE(x)$ as $lse_i(x)$. For clarity, this means that \n",
    "$\\begin{align}\n",
    "lse_i(x) = \\frac{e^{x_i}}{\\sum_{i = 1}^{n} e^{x_i}}\n",
    "\\end{align}$\n",
    "Now, we can make a useful simplification by saying that $a_i = e^{x_i}$ and that $A = \\sum_{i = 1}^{n} e^{x_i}$. With this notation, we have that\n",
    "$\\begin{align}\n",
    "lse_i(x) = \\frac{a_i}{A}\n",
    "\\end{align}$\n",
    "And then we have two cases. First, let us consider the case such that $i = j$. In this case, we have that\n",
    "$\\begin{align}\n",
    "\\frac{\\partial lse_i(x)}{\\partial x_i} & = \\frac{A \\frac{\\partial a_i}{\\partial x_i} - a_i \\frac{\\partial A}{\\partial x_i}}{A^2} \\\\\n",
    "& = \\frac{a_i}{A} - \\frac{a_i^2}{A^2}\n",
    "\\end{align}$\n",
    "And then if $i \\neq j$, we have that\n",
    "$\\begin{align}\n",
    "\\frac{\\partial lse_i(x)}{\\partial x_j} & = \\frac{A \\frac{\\partial a_i}{\\partial x_j} - a_i \\frac{\\partial A}{\\partial x_j}}{A^2} \\\\\n",
    "& = -\\frac{a_i a_j}{A^2}\n",
    "\\end{align}$\n",
    "Now, we want to write this in a more compact form. For simplicity again, let us call $a = (e^{x_1}, \\dots e^{x_n})$\n",
    "$\\begin{align}\n",
    "\\nabla^2LSE(x) & = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\left(\\sum_{i = 1}^n e^{x_i}\\right)\\textbf{diag}(a) - aa^T\\right] \\\\\n",
    "\\end{align}$\n",
    "Now, we want to show that this is positive semidefinite. To that end, let us consider the following vector $v \\in \\mathbb{R}^n$.\n",
    "$\\begin{align}\n",
    "v^T \\nabla^2LSE(x) v & = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\left(\\sum_{i = 1}^n e^{x_i}\\right)v^T\\textbf{diag}(a)v - v^Taa^Tv\\right] \\\\\n",
    "& = \\frac{1}{\\sum_{i = 1}^n e^{x_i}}\\left[\\sum_{i = 1}^n e^{x_i}v_i^2 - \\sum_{i = 1}^n \\sum_{j = 1}^n e^{x_i}e^{x_j}v_iv_j\\right] \\\\\n",
    "\\\\\n",
    "\\end{align}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
